{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ACKNOWLEDGEMENTS Special thanks to Peter Szmrecsanyi for authoring the original version of this hands-on lab, \"Deploying WebSphere Application Server on AIX using Ansible Automation\" for IBM Client Engineering for Systems (Montpellier). In addition to the embedded video, IBMers and Business Partners can also download the recording from Seismic . Automation of nearly any infrastructure endpoint using minimal amounts of code has immense practical value to a world increasingly dependent on clouds operated by different vendors, in varying countries, across multiple premises. Implementation of modern automation solutions can likewise translate to a number of benefits for businesses of every size: reduced storage and resource burden placed on the machines to be automated; a much smaller footprint on these endpoints that could be hacked or exploited by malicious users; and most importantly, a greatly simplified approach to automation in general. As environments change and operating systems advance over time, the automation jobs underway and supported by these technologies can be easily modified in lockstep as well. Adaptability and extensibility are key ingredients in the longevity of any technology \u2014 and fortunately, the automation tooling for the hybrid multicloud era has those in abundance. Red Hat Ansible Automation Platform (AAP) commodifies the automation of everything else that applications, services, and containers need to run upon. That may include infrastructure provisioning, server deployments, IoT edge devices, script execution, and lots of other things that operations teams spend their time doing to \u201ckeep the lights on.\u201d AAP makes automation available to everyone with the lightest touch of human-readable snippets of code. TERMINOLOGY Ansible and AAP may be used interchangeably \u2014 and will be throughout the course of this lab work. Who uses AAP today? Many different and potentially siloed personas: IT Operators : These include automation administrators responsible for ensuring that the automation platform is available to developers and implementors upstream within the organization. These individuals are generally concerned with the uptime of the automation platform, as any interruptions to service availability can directly impact upstream users. Platform Developers : These individuals are the automation \"plumbers\" who ensure that endpoints are viable for automation. Tasks and responsibilities include maintaining the AAP modules, plugins, and the Roles (content) to be used by platform users. They are the domain experts that are coding up collections which can later be extended or consumed by the end-users of the automation jobs. Platform Users : These are the automation \"writers,\" stitching together each automation task\u2014 play by play, task by task \u2014using content generated by the Platform Developers within their Playbooks. Essentially, this is configuration management applied to the operations of IT estates. Instead of having a hardcoded script that instructs exactly how to move from task A to task B, you are creating a Playbook that lays out the expected end state and asks AAP to figure out the delta between the current and end state. But critically, AAP will not move on to task B until all of the conditions required by task A have been satisfied. This is not only invaluable for debugging purposes; it also guarantees a consistency and predictability to how AAP's automation tasks will be performed \u2014 something that you will come to keenly appreciate throughout this hands-on material, as we incrementally add more tasks and automation jobs to Playbooks for execution. Together, these qualities have two profound implications for AAP clients. First, it removes any ambiguity from the automation process: AAP will execute your instructions in exactly the order you\u2019ve assigned. Second, it places the burden of deciding how to achieve the end states of tasks A and B on the automation engine itself, rather than requiring the user to explicitly define all the gory details themselves. When automation is made available to everyone, it becomes possible to automate everything. The series of hands-on tutorials and learning modules embedded in this Level 3 course are designed to provide IBM Sellers, IBM Technical Sellers, and Business Partners with the fluency to gain trusted advisor status with clients and the expertise to tailor live technological demonstrations for customers. Throughout Red Hat Ansible Automation Platform Level 3 for Sales and Technical Sales , you will utilize AAP in conjunction with IBM Power Systems infrastructure (PowerVC) to generate an IBM AIX-based virtual machine; subsequently, Ansible automation will be used to install and deploy a live WebSphere Application Server instance inside of the virtual machine. The various ways in which AAP's automation can also be applied to operational and administrative tasks\u2014 such as modifying root user characteristics, setting filesystem sizes, installing pre-requisites drivers and libraries, as well as software deployments \u2014will also be explored. All modules are accompanied by recordings and narrated instructions, delivered by your team of authors. It is strongly recommended that IBM Sellers and Technical Sellers watch these recordings, as they will be useful for you as you go about creating and recording your own Stand & Deliver presentations for Level 3 accreditation. LEVEL 3 ACCREDITATION To receive Level 3 accreditation, IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way in which participants will be evaluated before receiving accreditation \u2014differs depending on job role. Business Partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. IBM Sales and Technical Sales must develop and record a Stand & Deliver presentation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to achieve. The recording must cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques covered in this lab. Specific criteria that must be demonstrated as part of the Stand & Deliver recordings is provided within the documentation that accompanies the Level 3 course . LOOKING FOR COURSEWORK? The material covered in this hands-on lab is part of accredited learning and badging for IBMers and business partners. Shortcuts to the respective learning plans are available below: IBM | Sellers | Technical Sellers Business Partners | Sellers | Technical Sellers","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#red-hat-ansible-automation-platform-aap-commodifies-the-automation-of-everything-else-that-applications-services-and-containers-need-to-run-upon","text":"That may include infrastructure provisioning, server deployments, IoT edge devices, script execution, and lots of other things that operations teams spend their time doing to \u201ckeep the lights on.\u201d AAP makes automation available to everyone with the lightest touch of human-readable snippets of code. TERMINOLOGY Ansible and AAP may be used interchangeably \u2014 and will be throughout the course of this lab work. Who uses AAP today? Many different and potentially siloed personas: IT Operators : These include automation administrators responsible for ensuring that the automation platform is available to developers and implementors upstream within the organization. These individuals are generally concerned with the uptime of the automation platform, as any interruptions to service availability can directly impact upstream users. Platform Developers : These individuals are the automation \"plumbers\" who ensure that endpoints are viable for automation. Tasks and responsibilities include maintaining the AAP modules, plugins, and the Roles (content) to be used by platform users. They are the domain experts that are coding up collections which can later be extended or consumed by the end-users of the automation jobs. Platform Users : These are the automation \"writers,\" stitching together each automation task\u2014 play by play, task by task \u2014using content generated by the Platform Developers within their Playbooks. Essentially, this is configuration management applied to the operations of IT estates. Instead of having a hardcoded script that instructs exactly how to move from task A to task B, you are creating a Playbook that lays out the expected end state and asks AAP to figure out the delta between the current and end state. But critically, AAP will not move on to task B until all of the conditions required by task A have been satisfied. This is not only invaluable for debugging purposes; it also guarantees a consistency and predictability to how AAP's automation tasks will be performed \u2014 something that you will come to keenly appreciate throughout this hands-on material, as we incrementally add more tasks and automation jobs to Playbooks for execution. Together, these qualities have two profound implications for AAP clients. First, it removes any ambiguity from the automation process: AAP will execute your instructions in exactly the order you\u2019ve assigned. Second, it places the burden of deciding how to achieve the end states of tasks A and B on the automation engine itself, rather than requiring the user to explicitly define all the gory details themselves.","title":"Red Hat Ansible Automation Platform (AAP) commodifies the automation of everything else that applications, services, and containers need to run upon."},{"location":"#_2","text":"","title":""},{"location":"#when-automation-is-made-available-to-everyone-it-becomes-possible-to-automate-everything","text":"The series of hands-on tutorials and learning modules embedded in this Level 3 course are designed to provide IBM Sellers, IBM Technical Sellers, and Business Partners with the fluency to gain trusted advisor status with clients and the expertise to tailor live technological demonstrations for customers. Throughout Red Hat Ansible Automation Platform Level 3 for Sales and Technical Sales , you will utilize AAP in conjunction with IBM Power Systems infrastructure (PowerVC) to generate an IBM AIX-based virtual machine; subsequently, Ansible automation will be used to install and deploy a live WebSphere Application Server instance inside of the virtual machine. The various ways in which AAP's automation can also be applied to operational and administrative tasks\u2014 such as modifying root user characteristics, setting filesystem sizes, installing pre-requisites drivers and libraries, as well as software deployments \u2014will also be explored. All modules are accompanied by recordings and narrated instructions, delivered by your team of authors. It is strongly recommended that IBM Sellers and Technical Sellers watch these recordings, as they will be useful for you as you go about creating and recording your own Stand & Deliver presentations for Level 3 accreditation. LEVEL 3 ACCREDITATION To receive Level 3 accreditation, IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way in which participants will be evaluated before receiving accreditation \u2014differs depending on job role. Business Partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. IBM Sales and Technical Sales must develop and record a Stand & Deliver presentation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to achieve. The recording must cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques covered in this lab. Specific criteria that must be demonstrated as part of the Stand & Deliver recordings is provided within the documentation that accompanies the Level 3 course . LOOKING FOR COURSEWORK? The material covered in this hands-on lab is part of accredited learning and badging for IBMers and business partners. Shortcuts to the respective learning plans are available below: IBM | Sellers | Technical Sellers Business Partners | Sellers | Technical Sellers","title":"When automation is made available to everyone, it becomes possible to automate everything."},{"location":"HPCS/01-Introduction-to-HPCS-and-UKO/","text":"IBM Cloud Hyper Protect Crypto Services and Unified Key Orchestrator WAYS TO WATCH In addition to the embedded video, IBMers and Business Partners can also download the recording from Seismic . Welcome to the IBM Cloud Hyper Protect Crypto Services hands-on demonstration, where sellers and business partners will have the opportunity to test the functionality of the Unified Key Orchestrator across IBM Cloud and AWS environments. IBM Cloud Hyper Protect Crypto Services ( HPCS ) provides data encryption that\u2019s protected by a dedicated cloud hardware security module and enables multicloud key management. UKO, a component of HPCS, is what enables key orchestration across these multicloud environments. HPCS is built on FIPS 140-2 Level 4 certified hardware, the highest level in the industry. Unified Key Orchestrator ( UKO ) is a multi-cloud key management solution offered as a managed service on IBM Cloud. Built on \u2018Keep Your Own Key\u2019 (KYOK), UKO helps enterprises manage their data encryption keys over multiple key stores and across multiple clouds environments. These key environments include managed on-premises stores and keys on IBM Cloud, AWS, and Microsoft Azure. UKO manages and orchestrates these security policies from a single point of control. BRING YOUR OWN KEY vs. KEEP YOUR OWN KEY How does the \"Keep Your Own Key\" (KYOK) approach of UKO differ from \"Bring Your Own Key\" (BYOK)? BYOK is a way for clients to use their own keys to encrypt data. Key management services that provide BYOK are typically multi-tenant services. With these services, users can import encryption keys from on-premises hardware security modules (HSM) and then manage the keys. KYOK includes all of the capabilities of BYOK, but also provides technical assurances that IBM cannot access a client's keys. With KYOK, clients have exclusive control of the entire key hierarchy, including that of the master key. The following table provides further details on the differences between BYOK and KYOK: Key benefits to HPCS clients Lifecycle management for keys : A GUI and a REST API track keys as they progress. Deleted data is no longer retrievable, regardless of the application that stored it. Learn more. Encryption for IBM Cloud services : IBM Cloud services can integrate with this product. Clients receive a common-key-provider API for a consistent approach in IBM Cloud adoption. Learn more. Multicloud key management : Extend protection across cloud deployments. Manage all keys in one place, with added protection and simplicity. Learn more. Security certification : The service is built on FIPS 140-2 Level-4-certified hardware\u2014the highest offered by any cloud provider in the industry. Learn more. HSM control : Single-tenant, dedicated HSMs are controlled by you. IBM Cloud administrators have no access. Learn more. Key ceremony : IBM is the first to provide cloud command-line interface (smart cards) for the HSM key ceremony. Learn more. Next steps In the following section, you will need to review the pre-requisites and follow the provisioning instructions for setting up your IBM Technology Zone environment.","title":"Introduction to HPCS and UKO"},{"location":"HPCS/01-Introduction-to-HPCS-and-UKO/#_1","text":"","title":""},{"location":"HPCS/01-Introduction-to-HPCS-and-UKO/#ibm-cloud-hyper-protect-crypto-services-and-unified-key-orchestrator","text":"WAYS TO WATCH In addition to the embedded video, IBMers and Business Partners can also download the recording from Seismic . Welcome to the IBM Cloud Hyper Protect Crypto Services hands-on demonstration, where sellers and business partners will have the opportunity to test the functionality of the Unified Key Orchestrator across IBM Cloud and AWS environments. IBM Cloud Hyper Protect Crypto Services ( HPCS ) provides data encryption that\u2019s protected by a dedicated cloud hardware security module and enables multicloud key management. UKO, a component of HPCS, is what enables key orchestration across these multicloud environments. HPCS is built on FIPS 140-2 Level 4 certified hardware, the highest level in the industry. Unified Key Orchestrator ( UKO ) is a multi-cloud key management solution offered as a managed service on IBM Cloud. Built on \u2018Keep Your Own Key\u2019 (KYOK), UKO helps enterprises manage their data encryption keys over multiple key stores and across multiple clouds environments. These key environments include managed on-premises stores and keys on IBM Cloud, AWS, and Microsoft Azure. UKO manages and orchestrates these security policies from a single point of control. BRING YOUR OWN KEY vs. KEEP YOUR OWN KEY How does the \"Keep Your Own Key\" (KYOK) approach of UKO differ from \"Bring Your Own Key\" (BYOK)? BYOK is a way for clients to use their own keys to encrypt data. Key management services that provide BYOK are typically multi-tenant services. With these services, users can import encryption keys from on-premises hardware security modules (HSM) and then manage the keys. KYOK includes all of the capabilities of BYOK, but also provides technical assurances that IBM cannot access a client's keys. With KYOK, clients have exclusive control of the entire key hierarchy, including that of the master key. The following table provides further details on the differences between BYOK and KYOK:","title":"IBM Cloud Hyper Protect Crypto Services and Unified Key Orchestrator"},{"location":"HPCS/01-Introduction-to-HPCS-and-UKO/#_2","text":"","title":""},{"location":"HPCS/01-Introduction-to-HPCS-and-UKO/#key-benefits-to-hpcs-clients","text":"Lifecycle management for keys : A GUI and a REST API track keys as they progress. Deleted data is no longer retrievable, regardless of the application that stored it. Learn more. Encryption for IBM Cloud services : IBM Cloud services can integrate with this product. Clients receive a common-key-provider API for a consistent approach in IBM Cloud adoption. Learn more. Multicloud key management : Extend protection across cloud deployments. Manage all keys in one place, with added protection and simplicity. Learn more. Security certification : The service is built on FIPS 140-2 Level-4-certified hardware\u2014the highest offered by any cloud provider in the industry. Learn more. HSM control : Single-tenant, dedicated HSMs are controlled by you. IBM Cloud administrators have no access. Learn more. Key ceremony : IBM is the first to provide cloud command-line interface (smart cards) for the HSM key ceremony. Learn more.","title":"Key benefits to HPCS clients"},{"location":"HPCS/01-Introduction-to-HPCS-and-UKO/#_3","text":"","title":""},{"location":"HPCS/01-Introduction-to-HPCS-and-UKO/#next-steps","text":"In the following section, you will need to review the pre-requisites and follow the provisioning instructions for setting up your IBM Technology Zone environment.","title":"Next steps"},{"location":"HPCS/02-Provision-and-access/","text":"Provisioning an IBM Technology Zone environment IBM TECHNOLOGY ZONE You will require access to the IBM Technology Zone in order to reserve your environment and complete the hands-on training. If you do not yet have access or an account with the ITZ, you may register for one by visiting the following page: https://techzone.ibm.com In this section, you will provision and request access to an IBM Technology Zone (ITZ) environment where Hyper Protect Crypto Services (HPCS) and Unified Key Orchestrator (UKO) have been preconfigured for demonstration purposes. The keystores associated with this ITZ environment have already been linked to specific accounts that IBM administrates in Amazon Web Services (AWS) and Microsoft Azure public clouds. The hands-on environment can be provisioned free-of-charge using the reservation portal: https://techzone.ibm.com/collection/use-the-unified-key-orchestrator-uko-to-orchestrate-keys-across-aws-andor-azure/environments MULTIPLE ENVIRONMENT TEMPLATES ITZ will display two configurations to choose from. You will need to provision both the IBM Cloud HPCS and AWS Account Access tiles. The following instructions will guide you through how to do this. Select the Access to IBM Cloud HPCS tile by clicking the blue Reserve icon. You may select the option to Reserve now (recommended) or Schedule for later . In order to complete your ITZ reservation of the HPCS environment, you must supply the following information: Name : Give your reservation a unique name. Purpose : Set to Practice / Self-Education and affirm that the environment will not be used with customer data. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description : Provide a brief summary of how the environment will be used. Preferred Geography : US-East (VPC) End Date & Time : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . Reservations take approximately 15-20 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. WAIT UNTIL READY Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the HPCS instance. You will also receive an email to your inbox once the environment has successfully deployed. The \"Your environment is ready\" email contains links back to the My Reservations tab, which now is populated with all of the details needed to access your IBM Cloud HPCS environment. Repeat the same reservation Steps 2 through 6 for the AWS Account Access instance. You will need to wait for both instances (IBM HPCS and AWS) to finish provisioning through the My Reservations tab of the ITZ before continuing with the lab. Accessing the HPCS and AWS environments For the IBM Cloud HPCS environment, you will receive an email (see Step 6) with an invitation to connect your personal IBM Cloud account to an ITZ V2 account, as shown in the following screenshot. You must ACCEPT the invitation by clicking the Join Now button in the email. A web browser will open and direct you to log in to your IBM Cloud account. Proceed and then accept the ITZ V2 invitation when prompted to do so. Once authenticated and logged in, look for a drop-down in the top-right corner of the IBM Cloud interface and click it to toggle between different Organizations / Resource Groups. You should see ITZ V2 listed, potentially alongside multiple others. Click the ITZ V2 group and wait for the browser to refresh. From the left-hand interface, click the Resource List icon (three stacked and bulleted lines) and drill down in the Security row. You should see an HPCS instance similar to the one shown in the following screenshot. For retrieving the AWS Account Access that was requested in Step 7, return to your email inbox and look for an email stating that the AWS environment is ready (similar to the screenshot provided below). Record the AWS Access Group , AWS SSO User Portal URL , and Desktop details provided in the note. Alternatively, you can always retrieve this information from the My Reservations tab of the ITZ. Click the Desktop link in the email or navigate to https://techzone.awsapps.com/start to get started. When prompted, authenticate using your personal IBMid to continue to the AWS dashboard, as shown below. Click on the AWS Account (1) tile and drill down into the UKO User Management Console , which will redirect the web browser to a page resembling the following screenshot. Under the Console Home panel, multiple links are provided: Key Management Service , IAM , AWS Budgets , AWS Organizations , and so on. Select the Key Management Service link. From the drop-down in the top-right corner of the AWS dashboard, ensure that the US East (N. Virginia) region is selected. It should be set as such by default, but if it has been configured to something else, set it back to US East. CAUTION Do not modify or delete this key. If the key does not appear, verify that the US East (N. Virginia) region was selected in Step 17. If the key is still missing, reach out to the author of this documentation. Drill down into Key Management Service > Customer-managed keys : confirm that under the Aliases column is a key labelled TechZone-UKO-Key . Next steps Now that you have successfully provisioned and gained access to both IBM Cloud HPCS and AWS Key Management Service, we will next go ahead with generating your own key in AWS Keystore.","title":"Provision and access an IBM Technology Zone environment"},{"location":"HPCS/02-Provision-and-access/#_1","text":"","title":""},{"location":"HPCS/02-Provision-and-access/#provisioning-an-ibm-technology-zone-environment","text":"IBM TECHNOLOGY ZONE You will require access to the IBM Technology Zone in order to reserve your environment and complete the hands-on training. If you do not yet have access or an account with the ITZ, you may register for one by visiting the following page: https://techzone.ibm.com In this section, you will provision and request access to an IBM Technology Zone (ITZ) environment where Hyper Protect Crypto Services (HPCS) and Unified Key Orchestrator (UKO) have been preconfigured for demonstration purposes. The keystores associated with this ITZ environment have already been linked to specific accounts that IBM administrates in Amazon Web Services (AWS) and Microsoft Azure public clouds. The hands-on environment can be provisioned free-of-charge using the reservation portal: https://techzone.ibm.com/collection/use-the-unified-key-orchestrator-uko-to-orchestrate-keys-across-aws-andor-azure/environments MULTIPLE ENVIRONMENT TEMPLATES ITZ will display two configurations to choose from. You will need to provision both the IBM Cloud HPCS and AWS Account Access tiles. The following instructions will guide you through how to do this. Select the Access to IBM Cloud HPCS tile by clicking the blue Reserve icon. You may select the option to Reserve now (recommended) or Schedule for later . In order to complete your ITZ reservation of the HPCS environment, you must supply the following information: Name : Give your reservation a unique name. Purpose : Set to Practice / Self-Education and affirm that the environment will not be used with customer data. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description : Provide a brief summary of how the environment will be used. Preferred Geography : US-East (VPC) End Date & Time : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . Reservations take approximately 15-20 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. WAIT UNTIL READY Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the HPCS instance. You will also receive an email to your inbox once the environment has successfully deployed. The \"Your environment is ready\" email contains links back to the My Reservations tab, which now is populated with all of the details needed to access your IBM Cloud HPCS environment. Repeat the same reservation Steps 2 through 6 for the AWS Account Access instance. You will need to wait for both instances (IBM HPCS and AWS) to finish provisioning through the My Reservations tab of the ITZ before continuing with the lab.","title":"Provisioning an IBM Technology Zone environment"},{"location":"HPCS/02-Provision-and-access/#_2","text":"","title":""},{"location":"HPCS/02-Provision-and-access/#accessing-the-hpcs-and-aws-environments","text":"For the IBM Cloud HPCS environment, you will receive an email (see Step 6) with an invitation to connect your personal IBM Cloud account to an ITZ V2 account, as shown in the following screenshot. You must ACCEPT the invitation by clicking the Join Now button in the email. A web browser will open and direct you to log in to your IBM Cloud account. Proceed and then accept the ITZ V2 invitation when prompted to do so. Once authenticated and logged in, look for a drop-down in the top-right corner of the IBM Cloud interface and click it to toggle between different Organizations / Resource Groups. You should see ITZ V2 listed, potentially alongside multiple others. Click the ITZ V2 group and wait for the browser to refresh. From the left-hand interface, click the Resource List icon (three stacked and bulleted lines) and drill down in the Security row. You should see an HPCS instance similar to the one shown in the following screenshot. For retrieving the AWS Account Access that was requested in Step 7, return to your email inbox and look for an email stating that the AWS environment is ready (similar to the screenshot provided below). Record the AWS Access Group , AWS SSO User Portal URL , and Desktop details provided in the note. Alternatively, you can always retrieve this information from the My Reservations tab of the ITZ. Click the Desktop link in the email or navigate to https://techzone.awsapps.com/start to get started. When prompted, authenticate using your personal IBMid to continue to the AWS dashboard, as shown below. Click on the AWS Account (1) tile and drill down into the UKO User Management Console , which will redirect the web browser to a page resembling the following screenshot. Under the Console Home panel, multiple links are provided: Key Management Service , IAM , AWS Budgets , AWS Organizations , and so on. Select the Key Management Service link. From the drop-down in the top-right corner of the AWS dashboard, ensure that the US East (N. Virginia) region is selected. It should be set as such by default, but if it has been configured to something else, set it back to US East. CAUTION Do not modify or delete this key. If the key does not appear, verify that the US East (N. Virginia) region was selected in Step 17. If the key is still missing, reach out to the author of this documentation. Drill down into Key Management Service > Customer-managed keys : confirm that under the Aliases column is a key labelled TechZone-UKO-Key .","title":"Accessing the HPCS and AWS environments"},{"location":"HPCS/02-Provision-and-access/#_3","text":"","title":""},{"location":"HPCS/02-Provision-and-access/#next-steps","text":"Now that you have successfully provisioned and gained access to both IBM Cloud HPCS and AWS Key Management Service, we will next go ahead with generating your own key in AWS Keystore.","title":"Next steps"},{"location":"HPCS/03-Create-a-key/","text":"Generating a unique key with the AWS Keystore Previously, we provisioned and inspected encryption keys in AWS that were generated ahead of time by IBM Technology Zone. In the following section, you will generate an encryption key that can be later be used for managing resources accessed within the AWS environment. Return to the IBM Cloud dashboard and drill down into Resource List > Security , then click the name of the Hyper Protect Crypto Services-Techzone instance. The browser will redirect to a management panel for the HPCS instance. From the left-hand side of the interface, drill down into Vaults > Managed Keys and take note of the inventory of keys available. Two keys should be listed: TechZone-UKO-Key and UKO-Azure-Test-Key . Along the top-right corner of the Managed Keys table, locate and click the blue Create Key + button, as shown. SHARED ENVIRONMENT When selecting a Name for your key, be aware that other sellers and business partners enrolled in this Level 3 course will be able to see keys created in the shared HPCS environment. Do not use the names of clients or other confidential information as part of your key definition. You will be asked to supply the following information as part of the key definition: Specify the Target Vault : Tech Zone Vault Specify the Keystore Type : AWS Key Management Service Specify the Key Properties , which include: a unique Name ; State: Active ; and Expiration Date set three days into the future. Specify the Target Keystore : AWS Tech Zone Under the Summary tab, review the selections and when satisfied click the Create Key button. Validate encryption key generation on AWS Return to the AWS dashboard and open the Key Management Service (KMS) panel as before. From the left-side of the interface, drill down into the Customer-managed keys section. Examine the contents and look for the uniquely generated key that you created using the Unified Key Orchestrator (UKO) in Step 3. MISSING KEY If the key is not displayed, verify once again that your AWS dashboard is set to the US East (N. Virginia) region using the drop-down in the top-right corner of the interface. Test the key by encrypting AWS S3 bucket contents Locate the search bar at the top of the AWS dashboard. Search for s3 and click on the AWS S3 service result. Wait for the AWS S3 dashboard to load. From the left-hand side of the interface, drill down into Buckets and then click the orange Create Bucket button as shown. Provide the following details as you create a new S3 bucket: Name : Set to a unique identifier of your choosing. Region : Set to US East (N. Virginia) to match the region where your encryption key is located. Leave the ACLs and Bucket Versioning fields as Disabled . Encryption set to ENABLED with the AWS Key Management Service Key (SSE-KMS) option selected. Select the Choose from your AWS KMS keys option and specify the unique key generated in Step 5 from the dropdown menu. Note: The key will need to be identified by the KeyID field (found within the KMS Console in AWS); it will not be listed under the Name identifier you assigned upon key generation. Review the selections made and when satisfied click the Create Bucket option to continue. After creating the S3 bucket, the web browser will redirect back to the AWS S3 dashboard. Scroll down the page until you reach Buckets table and locate the newly-created bucket in the list. Click the name of the bucket to drill down further. With the bucket selected, click the orange Upload button and select a small file from your machine (one of the L3 lab documents, for example) to upload. No sensitive or confidential information, please. Wait for the asset to upload and be listed as an Object within the bucket. Click on the file name to see the object properties. To ensure that the file was properly uploaded and available for download, click on the Download button in the top-right corner of the interface. Open the file on your desktop to verify its integrity. Next steps With the AWS S3 bucket and encryption key now prepared, we will demonstrate how to digitally shred objects within AWS S3 using the HPCS Unified Key Orchestrator (UKO).","title":"Generating a unique key with the AWS Keystore"},{"location":"HPCS/03-Create-a-key/#_1","text":"","title":""},{"location":"HPCS/03-Create-a-key/#generating-a-unique-key-with-the-aws-keystore","text":"Previously, we provisioned and inspected encryption keys in AWS that were generated ahead of time by IBM Technology Zone. In the following section, you will generate an encryption key that can be later be used for managing resources accessed within the AWS environment. Return to the IBM Cloud dashboard and drill down into Resource List > Security , then click the name of the Hyper Protect Crypto Services-Techzone instance. The browser will redirect to a management panel for the HPCS instance. From the left-hand side of the interface, drill down into Vaults > Managed Keys and take note of the inventory of keys available. Two keys should be listed: TechZone-UKO-Key and UKO-Azure-Test-Key . Along the top-right corner of the Managed Keys table, locate and click the blue Create Key + button, as shown. SHARED ENVIRONMENT When selecting a Name for your key, be aware that other sellers and business partners enrolled in this Level 3 course will be able to see keys created in the shared HPCS environment. Do not use the names of clients or other confidential information as part of your key definition. You will be asked to supply the following information as part of the key definition: Specify the Target Vault : Tech Zone Vault Specify the Keystore Type : AWS Key Management Service Specify the Key Properties , which include: a unique Name ; State: Active ; and Expiration Date set three days into the future. Specify the Target Keystore : AWS Tech Zone Under the Summary tab, review the selections and when satisfied click the Create Key button.","title":"Generating a unique key with the AWS Keystore"},{"location":"HPCS/03-Create-a-key/#_2","text":"","title":""},{"location":"HPCS/03-Create-a-key/#validate-encryption-key-generation-on-aws","text":"Return to the AWS dashboard and open the Key Management Service (KMS) panel as before. From the left-side of the interface, drill down into the Customer-managed keys section. Examine the contents and look for the uniquely generated key that you created using the Unified Key Orchestrator (UKO) in Step 3. MISSING KEY If the key is not displayed, verify once again that your AWS dashboard is set to the US East (N. Virginia) region using the drop-down in the top-right corner of the interface.","title":"Validate encryption key generation on AWS"},{"location":"HPCS/03-Create-a-key/#_3","text":"","title":""},{"location":"HPCS/03-Create-a-key/#test-the-key-by-encrypting-aws-s3-bucket-contents","text":"Locate the search bar at the top of the AWS dashboard. Search for s3 and click on the AWS S3 service result. Wait for the AWS S3 dashboard to load. From the left-hand side of the interface, drill down into Buckets and then click the orange Create Bucket button as shown. Provide the following details as you create a new S3 bucket: Name : Set to a unique identifier of your choosing. Region : Set to US East (N. Virginia) to match the region where your encryption key is located. Leave the ACLs and Bucket Versioning fields as Disabled . Encryption set to ENABLED with the AWS Key Management Service Key (SSE-KMS) option selected. Select the Choose from your AWS KMS keys option and specify the unique key generated in Step 5 from the dropdown menu. Note: The key will need to be identified by the KeyID field (found within the KMS Console in AWS); it will not be listed under the Name identifier you assigned upon key generation. Review the selections made and when satisfied click the Create Bucket option to continue. After creating the S3 bucket, the web browser will redirect back to the AWS S3 dashboard. Scroll down the page until you reach Buckets table and locate the newly-created bucket in the list. Click the name of the bucket to drill down further. With the bucket selected, click the orange Upload button and select a small file from your machine (one of the L3 lab documents, for example) to upload. No sensitive or confidential information, please. Wait for the asset to upload and be listed as an Object within the bucket. Click on the file name to see the object properties. To ensure that the file was properly uploaded and available for download, click on the Download button in the top-right corner of the interface. Open the file on your desktop to verify its integrity.","title":"Test the key by encrypting AWS S3 bucket contents"},{"location":"HPCS/03-Create-a-key/#_4","text":"","title":""},{"location":"HPCS/03-Create-a-key/#next-steps","text":"With the AWS S3 bucket and encryption key now prepared, we will demonstrate how to digitally shred objects within AWS S3 using the HPCS Unified Key Orchestrator (UKO).","title":"Next steps"},{"location":"HPCS/04-Digitally-shred/","text":"Digitally shred AWS S3 bucket objects with Unified Key Orchestrator Owing to the steps taken in the previous section, all objects housed within in the AWS S3 bucket are now encrypted using a uniquely-generated encryption key. Recall that this key\u2014 which you defined \u2014was generated using the Unified Key Orchestrator (UKO) utility of IBM Cloud HPCS. By deactivating this key via UKO on IBM Cloud, any data encrypted with the key (including the bucket storage on AWS S3) will become inaccesible. In effect, disabling the encryption key will digitally shred all data encrypted with said key. This is incredibly useful for rendering inert and inaccessible vast amounts of encrypted data with a single action; far more expedient than seeking out and shredding (through some other process) assets on an individual basis. Return to the IBM Cloud dashboard and drill down into the HPCS UKO instance as done previously. From the HPCS UKO dashboard, navigate using the left-hand interface into Vaults > Managed Keys . Locate the unique encryption key that you generated in the previous section. At the far-right side of the key's row (adjacent the Length column) are three vertically stacked dots. Click the icon and then select Deactivate from the drop-down options. You will presented with two warning statements \u2014 click the checkbox next to each warning, and finally click the Deactivate Key button to finalize the request. DEACTIVATED KEYS Once the key has been deactivated, you will no longer see it listed within the Managed Keys section of the HPCS UKO dashboard. By default, the dashboard will hide all deactivated keys. However, the deactivated key can be viewed by adjusting the filter parameters. Click the X next to the tags along the top of the table to adjust the filters. Return to the AWS dashboard and drill down into the Key Management Service instance as done previously (search for KMS from the top search bar if you have difficulty locating it). From the left-hand side of the interface, drill down into the Customer-managed keys section. Note that the key generated by UKO previously now has a Status of Pending import . This confirms that the encryption key has been deactivated by UKO. KEY ID Recall from the previous section that the AWS Key Management Service lists customer-managed keys by Key ID hash strings, not the name of the key itself (which you specified when first generating the key). From the AWS dashboard, navigate back to the S3 buckets that were set up in the previous section. You will find that the object you uploaded earlier into the bucket is no longer accessible as a result of the key deactivation. Instead, your web browser will return an error similar to the one captured below. Next steps Before continuing with the lab, there are some loose ends to tie and tidying up to perform.","title":"Digitally shred AWS S3 objects"},{"location":"HPCS/04-Digitally-shred/#_1","text":"","title":""},{"location":"HPCS/04-Digitally-shred/#digitally-shred-aws-s3-bucket-objects-with-unified-key-orchestrator","text":"Owing to the steps taken in the previous section, all objects housed within in the AWS S3 bucket are now encrypted using a uniquely-generated encryption key. Recall that this key\u2014 which you defined \u2014was generated using the Unified Key Orchestrator (UKO) utility of IBM Cloud HPCS. By deactivating this key via UKO on IBM Cloud, any data encrypted with the key (including the bucket storage on AWS S3) will become inaccesible. In effect, disabling the encryption key will digitally shred all data encrypted with said key. This is incredibly useful for rendering inert and inaccessible vast amounts of encrypted data with a single action; far more expedient than seeking out and shredding (through some other process) assets on an individual basis. Return to the IBM Cloud dashboard and drill down into the HPCS UKO instance as done previously. From the HPCS UKO dashboard, navigate using the left-hand interface into Vaults > Managed Keys . Locate the unique encryption key that you generated in the previous section. At the far-right side of the key's row (adjacent the Length column) are three vertically stacked dots. Click the icon and then select Deactivate from the drop-down options. You will presented with two warning statements \u2014 click the checkbox next to each warning, and finally click the Deactivate Key button to finalize the request. DEACTIVATED KEYS Once the key has been deactivated, you will no longer see it listed within the Managed Keys section of the HPCS UKO dashboard. By default, the dashboard will hide all deactivated keys. However, the deactivated key can be viewed by adjusting the filter parameters. Click the X next to the tags along the top of the table to adjust the filters. Return to the AWS dashboard and drill down into the Key Management Service instance as done previously (search for KMS from the top search bar if you have difficulty locating it). From the left-hand side of the interface, drill down into the Customer-managed keys section. Note that the key generated by UKO previously now has a Status of Pending import . This confirms that the encryption key has been deactivated by UKO. KEY ID Recall from the previous section that the AWS Key Management Service lists customer-managed keys by Key ID hash strings, not the name of the key itself (which you specified when first generating the key). From the AWS dashboard, navigate back to the S3 buckets that were set up in the previous section. You will find that the object you uploaded earlier into the bucket is no longer accessible as a result of the key deactivation. Instead, your web browser will return an error similar to the one captured below.","title":"Digitally shred AWS S3 bucket objects with Unified Key Orchestrator"},{"location":"HPCS/04-Digitally-shred/#_2","text":"","title":""},{"location":"HPCS/04-Digitally-shred/#next-steps","text":"Before continuing with the lab, there are some loose ends to tie and tidying up to perform.","title":"Next steps"},{"location":"HPCS/05-Clean-up/","text":"Clean up and conclusion TIDYING UP These environments DO NOT automatically reset. Please ensure you have removed all your keys and adjustments before continuing. Congratulations, you've reached the conclusion of the Hyper Protect Crypto Services (HPCS) Unified Key Orchestration (UKO) demonstration. However, as this environment is multi-tenant and for use by multiple IBMers and business partners, we kindly ask that you take the time to clean up changes you've introduced into the environment before moving on with the other lab modules. IBMers : Before wiping your environment, we recommend taking the time to rehearse and record your Stand and Deliver presentation. As a reminder, in the IBM Cloud HPCS environment you should: Deactivate the encryption keys you have created using UKO. Destroy these keys once they have been deactivated. This will permanently remove them from the environment. DO NOT DELETE KEYS IN THE AWS ENVIRONMENT Only delete keys on the IBM Cloud HPCS environment. These changes will automatically be propagated to the AWS S3 and Key Management Service. Remove these keys from the HPCS UKO vault. Furthermore, if you created any S3 buckets in the AWS environment you should: Delete any S3 buckets you created from the S3 service. A video walkthrough for how to perform these steps is also included within the \"See It\" replays included in the lab.","title":"Clean up and conclusion"},{"location":"HPCS/05-Clean-up/#_1","text":"","title":""},{"location":"HPCS/05-Clean-up/#clean-up-and-conclusion","text":"TIDYING UP These environments DO NOT automatically reset. Please ensure you have removed all your keys and adjustments before continuing. Congratulations, you've reached the conclusion of the Hyper Protect Crypto Services (HPCS) Unified Key Orchestration (UKO) demonstration. However, as this environment is multi-tenant and for use by multiple IBMers and business partners, we kindly ask that you take the time to clean up changes you've introduced into the environment before moving on with the other lab modules.","title":"Clean up and conclusion"},{"location":"HPCS/05-Clean-up/#_2","text":"","title":""},{"location":"HPCS/05-Clean-up/#ibmers-before-wiping-your-environment-we-recommend-taking-the-time-to-rehearse-and-record-your-stand-and-deliver-presentation","text":"As a reminder, in the IBM Cloud HPCS environment you should: Deactivate the encryption keys you have created using UKO. Destroy these keys once they have been deactivated. This will permanently remove them from the environment. DO NOT DELETE KEYS IN THE AWS ENVIRONMENT Only delete keys on the IBM Cloud HPCS environment. These changes will automatically be propagated to the AWS S3 and Key Management Service. Remove these keys from the HPCS UKO vault. Furthermore, if you created any S3 buckets in the AWS environment you should: Delete any S3 buckets you created from the S3 service. A video walkthrough for how to perform these steps is also included within the \"See It\" replays included in the lab.","title":"IBMers: Before wiping your environment, we recommend taking the time to rehearse and record your Stand and Deliver presentation."},{"location":"HPVS/01-Introduction-to-HPVS/","text":"IBM Cloud Hyper Protect Virtual Servers for VPC WATCH THIS DEMONSTRATION Follow the link to learn more about Confidential Computing for a financial transaction using HPVS. In this section, you will be guided through the steps involved with configuring an IBM Hyper Protect Virtual Server for Virtual Private Cloud (VPC) instance\u2014 which will from this point onward be referred to as HPVS \u2014 on IBM Cloud. Afterwards, you will be provided the instructions needed to reproduce a simulated deployment of a fictitious PayNow application. This process will require the use of Git, Terraform, Hyper Protect \"Contracts\", and navigation of the IBM Cloud interface. HPVS provides a Confidential Computing solution for compute instances in the public cloud. The result is that data within a Hyper Protect instance remains protected in all three stages of the data lifecycle: at-rest, in-flight, AND in-memory / in-use. This is achieved with the technical assurance that workloads and applications can run in the public cloud with full peace of mind for clients. In the following sections, sellers and business partners will learn: How to configure and provision an HPVS instance on IBM Cloud How Contracts are needed to create an HPVS instance with the IBM Hyper Protect Container Runtime (HPCR) image How to deploy a sample application on HPVS COSTS Due to costs that potentially may be incurred from deploying a live HPVS instance on IBM Cloud\u2014 and because sellers and partners must use their own personal IBM Cloud accounts for previewing the IBM Cloud catalog \u2014it is recommended that those taking this course DO NOT deploy any HPVS instances for this Level 3 training, unless: You have a valid client opportunity number, in which case you may use that code to apply for a Hyper Protect Services \"Proof of Concept\" engagement \u2014 which will fund the demo environment for your client opportunity. Follow the procedures outlined HERE for how to apply. You have access to an IBM internal paid account for IBM Cloud, in which case billing will be charged to your respective IBM department, instead of your personal account. Follow the procedures outlined HERE for how to apply. You are willing to accept the charges to your personal IBM account and the credit card associated with that account. The IBM enablement team cannot reimburse such costs and any costs taken on by the IBM account are the responsibility of the user. In lieu of a live production environment, sellers will be furnished with the instruction sets necessary to reproduce these demonstrations as part of a funded client demonstration. Sales opportunity numbers can be applied towards Proof of Experience (PoX) engagements. Documentation for how to do so is included as a part of the coursework resources. IBM sellers and technical sellers , who are required to produce a Stand & Deliver recording of the hands-on demonstration as part of their accreditation process, you will NOT be required to show (on screen) anything beyond the initial configuration steps for Hyper Protect Virtual Servers for VPC on IBM Cloud. Do not deploy the instance, unless you are prepared to finance it on your personal IBM Cloud account. Furthermore, you are not required to record any of the steps involving the hypothetical PayNow application, as doing so would require deployment of a live instance. IBM business partners , who receive accreditation via a passing grade on a multiple-choice test, will only need to answer questions based on the initial configuration steps of Hyper Protect Virtual Servers for VPC on IBM Cloud. Quiz questions will not cover the screen readouts or other material involving either the PayNow application or a fully deployed instance. As mentioned previously, do not deploy the IBM Cloud instance into a live state unless you are prepared to self-finance any potential billing on your personal IBM Cloud account. Additional HPVS resources that may be beneficial to seller and business partners are available online: Sales Kit (Seismic) for HPVS Whitepaper (PDF): \"The Second Generation of IBM Hyper Protect Platform\" Next steps In the following section, you will need to review the pre-requisites and follow the provisioning instructions for setting up your IBM Cloud and local environments.","title":"Introduction to HPVS"},{"location":"HPVS/01-Introduction-to-HPVS/#_1","text":"","title":""},{"location":"HPVS/01-Introduction-to-HPVS/#ibm-cloud-hyper-protect-virtual-servers-for-vpc","text":"WATCH THIS DEMONSTRATION Follow the link to learn more about Confidential Computing for a financial transaction using HPVS. In this section, you will be guided through the steps involved with configuring an IBM Hyper Protect Virtual Server for Virtual Private Cloud (VPC) instance\u2014 which will from this point onward be referred to as HPVS \u2014 on IBM Cloud. Afterwards, you will be provided the instructions needed to reproduce a simulated deployment of a fictitious PayNow application. This process will require the use of Git, Terraform, Hyper Protect \"Contracts\", and navigation of the IBM Cloud interface. HPVS provides a Confidential Computing solution for compute instances in the public cloud. The result is that data within a Hyper Protect instance remains protected in all three stages of the data lifecycle: at-rest, in-flight, AND in-memory / in-use. This is achieved with the technical assurance that workloads and applications can run in the public cloud with full peace of mind for clients. In the following sections, sellers and business partners will learn: How to configure and provision an HPVS instance on IBM Cloud How Contracts are needed to create an HPVS instance with the IBM Hyper Protect Container Runtime (HPCR) image How to deploy a sample application on HPVS COSTS Due to costs that potentially may be incurred from deploying a live HPVS instance on IBM Cloud\u2014 and because sellers and partners must use their own personal IBM Cloud accounts for previewing the IBM Cloud catalog \u2014it is recommended that those taking this course DO NOT deploy any HPVS instances for this Level 3 training, unless: You have a valid client opportunity number, in which case you may use that code to apply for a Hyper Protect Services \"Proof of Concept\" engagement \u2014 which will fund the demo environment for your client opportunity. Follow the procedures outlined HERE for how to apply. You have access to an IBM internal paid account for IBM Cloud, in which case billing will be charged to your respective IBM department, instead of your personal account. Follow the procedures outlined HERE for how to apply. You are willing to accept the charges to your personal IBM account and the credit card associated with that account. The IBM enablement team cannot reimburse such costs and any costs taken on by the IBM account are the responsibility of the user. In lieu of a live production environment, sellers will be furnished with the instruction sets necessary to reproduce these demonstrations as part of a funded client demonstration. Sales opportunity numbers can be applied towards Proof of Experience (PoX) engagements. Documentation for how to do so is included as a part of the coursework resources. IBM sellers and technical sellers , who are required to produce a Stand & Deliver recording of the hands-on demonstration as part of their accreditation process, you will NOT be required to show (on screen) anything beyond the initial configuration steps for Hyper Protect Virtual Servers for VPC on IBM Cloud. Do not deploy the instance, unless you are prepared to finance it on your personal IBM Cloud account. Furthermore, you are not required to record any of the steps involving the hypothetical PayNow application, as doing so would require deployment of a live instance. IBM business partners , who receive accreditation via a passing grade on a multiple-choice test, will only need to answer questions based on the initial configuration steps of Hyper Protect Virtual Servers for VPC on IBM Cloud. Quiz questions will not cover the screen readouts or other material involving either the PayNow application or a fully deployed instance. As mentioned previously, do not deploy the IBM Cloud instance into a live state unless you are prepared to self-finance any potential billing on your personal IBM Cloud account. Additional HPVS resources that may be beneficial to seller and business partners are available online: Sales Kit (Seismic) for HPVS Whitepaper (PDF): \"The Second Generation of IBM Hyper Protect Platform\"","title":"IBM Cloud Hyper Protect Virtual Servers for VPC"},{"location":"HPVS/01-Introduction-to-HPVS/#_2","text":"","title":""},{"location":"HPVS/01-Introduction-to-HPVS/#next-steps","text":"In the following section, you will need to review the pre-requisites and follow the provisioning instructions for setting up your IBM Cloud and local environments.","title":"Next steps"},{"location":"HPVS/02-Prerequites-and-Preparation/","text":"Prerequisites and Preparation Sign up for an IBM Cloud account and IBMid using the following link: https://cloud.ibm.com/registration You will be asked to supply a credit card to be associated with the account. Any billing you accrue on your IBM Cloud account will be billed at the next IBM Cloud billing cycle to that credit card. On your local desktop or laptop, download and install Git . Follow the instruction set appropriate to your operating system. macOS Open the macOS Terminal shell prompt. Execute the following instructions to install the Homebrew package manager: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Follow the instructions provided during the Homebrew installation to add Homebrew to your machine's local PATH. Once the installation of Homebrew is complete, execute the following command to install Git: brew install git After installation has concluded, verify the Git install using the following statement: git version WINDOWS Visit the Git for Windows page and download the latest version to your machine. Once the installer has started, follow the instructions as provided in the Git Setup wizard screen until the installation is complete. Open the windows command prompt (or Git Bash if you selected not to use the standard Git Windows Command Prompt during the Git installation). Execute git version to verify Git was installed. Install the IBM Cloud Command Line Interface (CLI) for your local machine to manage resources in the IBM Cloud. Instructions for how to do so across a variety of operating systems are provided online: https://cloud.ibm.com/docs/cli?topic=cli-install-ibmcloud-cli Create a public SSH key which will be used for connecting to the virtual server instance that we will deploy later. Within your IBM Cloud CLI or local Terminal command prompt, execute the following command to generate a public SSH key: ssh-keygen -t rsa The command will generate two files\u2014 id_rsa and id_rsa.pub \u2014which will be placed either under an .ssh directory (the exact directory address will be displayed to the command line, such as /Users/.ssh/id_rsa ) or within the same directory that you executed the ssh-key -t rsa instruction from. Execute open . in the command line to open the current directory with your desktop file browser and look for the pair of key files. From the IBM Cloud web interface, provision a Lite plan of the IBM Log Analysis service from the following page: https://cloud.ibm.com/catalog/services/logdna?callback=%2Fobserve%2Flogging%2Fcreate Note that the \"Lite\" plan is free to use, but with limited functionality and only a 30 day window to operate before it is automatically de-provisioned. When configuring the service, use the following options: Service Name : Set to a unique name of your preference. Resource Group : Leave as default. Tags and Access Management tags can be left blank. Confirm that you have read and agree to the license agreements, then click Create to provision. Install the OpenSSL binary on your local machine. More information about the OpenSSL Project can be found within their online documentation: https://www.openssl.org To install OpenSSL, either read the instructions detailed here ( https://www.openssl.org/source/gitrepo.html ) or execute the following commands one at a time in your command line interface: git clone git://git.openssl.org/openssl.git cd openssl git config core.autocrlf false git config core.eol lf git checkout . Install the Terraform CLI on your local machine. Detailed instructions and video instructions for doing so are provided online: https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli You may either follow the detailed instructions (linked above) or execute the condensed instructions (line by line) below: brew tap hashicorp/tap brew install hashicorp/tap/terraform brew update brew upgrade hashicorp/tap/terraform terraform -version Next steps In the following section, you will use these tools and technologies to generate a Contract for creation of an HPVS instance with the IBM Hyper Protect Container Runtime (HPCR) image.","title":"Prerequisites and Preparation"},{"location":"HPVS/02-Prerequites-and-Preparation/#_1","text":"","title":""},{"location":"HPVS/02-Prerequites-and-Preparation/#prerequisites-and-preparation","text":"Sign up for an IBM Cloud account and IBMid using the following link: https://cloud.ibm.com/registration You will be asked to supply a credit card to be associated with the account. Any billing you accrue on your IBM Cloud account will be billed at the next IBM Cloud billing cycle to that credit card. On your local desktop or laptop, download and install Git . Follow the instruction set appropriate to your operating system. macOS Open the macOS Terminal shell prompt. Execute the following instructions to install the Homebrew package manager: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Follow the instructions provided during the Homebrew installation to add Homebrew to your machine's local PATH. Once the installation of Homebrew is complete, execute the following command to install Git: brew install git After installation has concluded, verify the Git install using the following statement: git version WINDOWS Visit the Git for Windows page and download the latest version to your machine. Once the installer has started, follow the instructions as provided in the Git Setup wizard screen until the installation is complete. Open the windows command prompt (or Git Bash if you selected not to use the standard Git Windows Command Prompt during the Git installation). Execute git version to verify Git was installed. Install the IBM Cloud Command Line Interface (CLI) for your local machine to manage resources in the IBM Cloud. Instructions for how to do so across a variety of operating systems are provided online: https://cloud.ibm.com/docs/cli?topic=cli-install-ibmcloud-cli Create a public SSH key which will be used for connecting to the virtual server instance that we will deploy later. Within your IBM Cloud CLI or local Terminal command prompt, execute the following command to generate a public SSH key: ssh-keygen -t rsa The command will generate two files\u2014 id_rsa and id_rsa.pub \u2014which will be placed either under an .ssh directory (the exact directory address will be displayed to the command line, such as /Users/.ssh/id_rsa ) or within the same directory that you executed the ssh-key -t rsa instruction from. Execute open . in the command line to open the current directory with your desktop file browser and look for the pair of key files. From the IBM Cloud web interface, provision a Lite plan of the IBM Log Analysis service from the following page: https://cloud.ibm.com/catalog/services/logdna?callback=%2Fobserve%2Flogging%2Fcreate Note that the \"Lite\" plan is free to use, but with limited functionality and only a 30 day window to operate before it is automatically de-provisioned. When configuring the service, use the following options: Service Name : Set to a unique name of your preference. Resource Group : Leave as default. Tags and Access Management tags can be left blank. Confirm that you have read and agree to the license agreements, then click Create to provision. Install the OpenSSL binary on your local machine. More information about the OpenSSL Project can be found within their online documentation: https://www.openssl.org To install OpenSSL, either read the instructions detailed here ( https://www.openssl.org/source/gitrepo.html ) or execute the following commands one at a time in your command line interface: git clone git://git.openssl.org/openssl.git cd openssl git config core.autocrlf false git config core.eol lf git checkout . Install the Terraform CLI on your local machine. Detailed instructions and video instructions for doing so are provided online: https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli You may either follow the detailed instructions (linked above) or execute the condensed instructions (line by line) below: brew tap hashicorp/tap brew install hashicorp/tap/terraform brew update brew upgrade hashicorp/tap/terraform terraform -version","title":"Prerequisites and Preparation"},{"location":"HPVS/02-Prerequites-and-Preparation/#_2","text":"","title":""},{"location":"HPVS/02-Prerequites-and-Preparation/#next-steps","text":"In the following section, you will use these tools and technologies to generate a Contract for creation of an HPVS instance with the IBM Hyper Protect Container Runtime (HPCR) image.","title":"Next steps"}]}